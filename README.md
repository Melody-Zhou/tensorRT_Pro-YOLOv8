
## ç®€ä»‹

è¯¥ä»“åº“åŸºäº [shouxieai/tensorRT_Pro](https://github.com/shouxieai/tensorRT_Pro)ï¼Œå¹¶è¿›è¡Œäº†è°ƒæ•´ä»¥æ”¯æŒ YOLOv8 çš„å„é¡¹ä»»åŠ¡ã€‚

* ç›®å‰å·²æ”¯æŒ YOLOv8ã€YOLOv8-Clsã€YOLOv8-Segã€YOLOv8-OBBã€YOLOv8-Poseã€RT-DETRã€ByteTrackã€YOLOv9ã€YOLOv10ã€RTMOã€PP-OCRv4ã€LaneATTã€CLRNetã€CLRerNetã€YOLO11ã€Depth-Anythingã€YOLOv12 é«˜æ€§èƒ½æ¨ç†ï¼ï¼ï¼ğŸš€ğŸš€ğŸš€
* åŸºäº tensorRT8.xï¼ŒC++ é«˜çº§æ¥å£ï¼ŒC++ éƒ¨ç½²ï¼ŒæœåŠ¡å™¨/åµŒå…¥å¼ä½¿ç”¨

<div align=center><img src="./assets/output.jpg" width="50%" height="50%"></div>

## CSDNæ–‡ç« åŒæ­¥è®²è§£
- ğŸ”¥ [YOLOv8æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134276907)
- ğŸ”¥ [YOLOv8-Clsæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134277392)
- ğŸ”¥ [YOLOv8-Segæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134277752)
- ğŸ”¥ [YOLOv8-OBBæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/135713830)
- ğŸ”¥ [YOLOv8-Poseæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134278117)
- ğŸ”¥ [RT-DETRæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134356250)
- ğŸ”¥ [YOLOv9æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/136492338)
- ğŸ”¥ [YOLOv10æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/139216405)
- ğŸ”¥ [MMPose-RTMOæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸Šï¼‰](https://blog.csdn.net/qq_40672115/article/details/139364023)
- ğŸ”¥ [MMPose-RTMOæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸‹ï¼‰](https://blog.csdn.net/qq_40672115/article/details/139375752)
- ğŸ”¥ [LayerNorm Pluginçš„ä½¿ç”¨ä¸è¯´æ˜](https://blog.csdn.net/qq_40672115/article/details/140246052)
- ğŸ”¥ [PaddleOCR-PP-OCRv4æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸Šï¼‰](https://blog.csdn.net/qq_40672115/article/details/140571346)
- ğŸ”¥ [PaddleOCR-PP-OCRv4æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸­ï¼‰](https://blog.csdn.net/qq_40672115/article/details/140585830)
- ğŸ”¥ [PaddleOCR-PP-OCRv4æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸‹ï¼‰](https://blog.csdn.net/qq_40672115/article/details/140648937)
- ğŸ”¥ [LaneATTæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸Šï¼‰](https://blog.csdn.net/qq_40672115/article/details/140891544)
- ğŸ”¥ [LaneATTæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸‹ï¼‰](https://blog.csdn.net/qq_40672115/article/details/140909528)
- ğŸ”¥ [CLRNetæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸Šï¼‰](https://blog.csdn.net/qq_40672115/article/details/141090952)
- ğŸ”¥ [CLRNetæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸‹ï¼‰](https://blog.csdn.net/qq_40672115/article/details/141107365)
- ğŸ”¥ [CLRerNetæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸Šï¼‰](https://blog.csdn.net/qq_40672115/article/details/141275384)
- ğŸ”¥ [CLRerNetæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸‹ï¼‰](https://blog.csdn.net/qq_40672115/article/details/141275949)
- ğŸ”¥ [YOLO11æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/143089165)
- ğŸ”¥ [Depth-Anythingæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸Šï¼‰](https://blog.csdn.net/qq_40672115/article/details/144199266)
- ğŸ”¥ [Depth-Anythingæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸‹ï¼‰](https://blog.csdn.net/qq_40672115/article/details/144475226)
- ğŸ”¥ [YOLOv12æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/145738637)

## Top News
- **2025/2/19**
  - YOLOv12 æ”¯æŒ
- **2024/12/14**
  - Depth-Anything æ”¯æŒ
- **2024/10/20**
  - YOLO11 åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ã€å§¿æ€ç‚¹ä¼°è®¡ä»»åŠ¡æ”¯æŒ
- **2024/8/18**
  - CLRerNet æ”¯æŒ
- **2024/8/11**
  - CLRNet æ”¯æŒ
- **2024/8/4**
  - LaneATT æ”¯æŒ
  - æä¾›æµ‹è¯•è§†é¢‘ä¸‹è½½ï¼ˆ[Baidu Drive](https://pan.baidu.com/s/1g-DvhZSIbXhEqp4iiFANTQ?pwd=lane )ï¼‰
- **2024/7/24**
  - PP-OCRv4 æ”¯æŒ
  - cuOSD æ”¯æŒï¼Œä»£ç  copy è‡ª [Lidar_AI_Solution/libraries/cuOSD](https://github.com/NVIDIA-AI-IOT/Lidar_AI_Solution/tree/master/libraries/cuOSD)
- **2024/7/7**
  - LayerNorm Plugin æ”¯æŒï¼Œä»£ç  copy è‡ª [CUDA-BEVFusion/src/plugins/custom_layernorm.cu](https://github.com/NVIDIA-AI-IOT/Lidar_AI_Solution/blob/master/CUDA-BEVFusion/src/plugins/custom_layernorm.cu)
  - æä¾› ONNX æ¨¡å‹ä¸‹è½½ï¼ˆ[Baidu Drive](https://pan.baidu.com/s/1MbPYzUEkONsjCPOudiTt1A?pwd=onnx)ï¼‰ï¼Œæ–¹ä¾¿å¤§å®¶æµ‹è¯•ä½¿ç”¨
- **2024/6/1**
  - RTMO æ”¯æŒ
- **2024/5/29**
  - ä¿®æ”¹ YOLOv6 çš„ ONNX å¯¼å‡ºä»¥åŠæ¨ç†
- **2024/5/26**
  - YOLOv10 æ”¯æŒ
- **2024/3/5**
  - YOLOv9 æ”¯æŒ
- **2024/2/1**
  - æ–°å¢ MinMaxCalibrator æ ¡å‡†å™¨ï¼Œå¯ä»¥é€šè¿‡ `TRT::Calibrator::MinMax` æŒ‡å®š
  - æ–°å¢ mAP æµ‹è¯•ä½¿ç”¨çš„ä¸€äº›è„šæœ¬æ–‡ä»¶ï¼ŒmAP è®¡ç®—ä»£ç  copy è‡ª [yolov6/core/evaler.py#L231](https://github.com/meituan/YOLOv6/blob/main/yolov6/core/evaler.py#L231)
- **2024/1/21**
  - YOLOv8-OBB æ”¯æŒ
  - ByteTrack æ”¯æŒï¼Œå®ç°åŸºæœ¬è·Ÿè¸ªåŠŸèƒ½
- **2024/1/10**
  - ä¿®å¤ IoU è®¡ç®— bug
- **2023/11/12**
  - RT-DETR æ”¯æŒ
- **2023/11/07**
  - é¦–æ¬¡æäº¤ä»£ç ï¼ŒYOLOv8 åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ã€å§¿æ€ç‚¹ä¼°è®¡ä»»åŠ¡æ”¯æŒ

## ç¯å¢ƒé…ç½®

è¯¥é¡¹ç›®ä¾èµ–äº cudaã€cudnnã€tensorRTã€opencvã€protobuf åº“ï¼Œè¯·åœ¨ CMakeLists.txt æˆ– Makefile ä¸­æ‰‹åŠ¨æŒ‡å®šè·¯å¾„é…ç½®

* æœåŠ¡å™¨
  * CUDA >= 10.2
  * cuDNN >= 8.x
  * TensorRT >= 8.x
  * protobuf == 3.11.4
  * è½¯ä»¶å®‰è£…è¯·å‚è€ƒï¼š[Ubuntu20.04è½¯ä»¶å®‰è£…å¤§å…¨](https://blog.csdn.net/qq_40672115/article/details/130255299)
* åµŒå…¥å¼
  * jetpack >= 4.6
  * protobuf == 3.11.4

å…‹éš†è¯¥é¡¹ç›®

```shell
git clone https://github.com/Melody-Zhou/tensorRT_Pro-YOLOv8.git
```

<details>
<summary>CMakeLists.txt ç¼–è¯‘</summary>

1. ä¿®æ”¹åº“æ–‡ä»¶è·¯å¾„

```cmake
# CMakeLists.txt 13 è¡Œ, ä¿®æ”¹ opencv è·¯å¾„
set(OpenCV_DIR   "/usr/local/include/opencv4/")

# CMakeLists.txt 15 è¡Œ, ä¿®æ”¹ cuda è·¯å¾„
set(CUDA_TOOLKIT_ROOT_DIR     "/usr/local/cuda-11.6")

# CMakeLists.txt 16 è¡Œ, ä¿®æ”¹ cudnn è·¯å¾„
set(CUDNN_DIR    "/usr/local/cudnn8.4.0.27-cuda11.6")

# CMakeLists.txt 17 è¡Œ, ä¿®æ”¹ tensorRT è·¯å¾„
set(TENSORRT_DIR "/opt/TensorRT-8.4.1.5")

# CMakeLists.txt 20 è¡Œ, ä¿®æ”¹ protobuf è·¯å¾„
set(PROTOBUF_DIR "/home/jarvis/protobuf")
```
2. ç¼–è¯‘

```shell
mkdir build
cd build
cmake ..
make -j64
```

</details>

<details>
<summary>Makefile ç¼–è¯‘</summary>

1. ä¿®æ”¹åº“æ–‡ä»¶è·¯å¾„

```makefile
# Makefile 4 è¡Œï¼Œä¿®æ”¹ protobuf è·¯å¾„
lean_protobuf  := /home/jarvis/protobuf

# Makefile 5 è¡Œï¼Œä¿®æ”¹ tensorRT è·¯å¾„
lean_tensor_rt := /opt/TensorRT-8.4.1.5

# Makefile 6 è¡Œï¼Œä¿®æ”¹ cudnn è·¯å¾„
lean_cudnn     := /usr/local/cudnn8.4.0.27-cuda11.6

# Makefile 7 è¡Œï¼Œä¿®æ”¹ opencv è·¯å¾„
lean_opencv    := /usr/local

# Makefile 8 è¡Œï¼Œä¿®æ”¹ cuda è·¯å¾„
lean_cuda      := /usr/local/cuda-11.6
```

2. ç¼–è¯‘

```shell
make -j64
```

</details>

## å„é¡¹ä»»åŠ¡æ”¯æŒ

<details>
<summary>YOLOv3æ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv3

```shell
git clone https://github.com/ultralytics/yolov3.git
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== export.py ==========

# yolov3/export.pyç¬¬160è¡Œ
# output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#         elif isinstance(model, DetectionModel):
#             dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output']            
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1,25200,85)
```

3. å¯¼å‡º onnx æ¨¡å‹

```shell
cd yolov3
python export.py --weights=yolov3.pt --dynamic --simplify --include=onnx --opset=11
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp yolov3/yolov3.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8

# ä¿®æ”¹ä»£ç åœ¨ src/application/app_yolo.cpp: app_yolo å‡½æ•°ä¸­, ä½¿ç”¨ V3 çš„æ–¹å¼å³å¯è¿è¡Œ
# test(Yolo::Type::V3, TRT::Mode::FP32, "yolov3");

make yolo -j64
```

</details>

<details>
<summary>YOLOXæ”¯æŒ</summary>

1. ä¸‹è½½ YOLOX

```shell
git clone https://github.com/Megvii-BaseDetection/YOLOX.git
```

2. å¯¼å‡º onnx æ¨¡å‹

```shell
cd YOLOX
export PYTHONPATH=$PYTHONPATH:.
python tools/export_onnx.py -c yolox_s.pth -f exps/default/yolox_s.py --output-name=yolox_s.onnx --dynamic --decode_in_inference
```

3. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp YOLOX/yolox_s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8

# ä¿®æ”¹ä»£ç åœ¨ src/application/app_yolo.cpp: app_yolo å‡½æ•°ä¸­, ä½¿ç”¨ X çš„æ–¹å¼å³å¯è¿è¡Œ
# test(Yolo::Type::X, TRT::Mode::FP32, "yolox_s");

make yolo -j64
```

</details>

<details>
<summary>YOLOv5æ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv5

```shell
git clone https://github.com/ultralytics/yolov5.git
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== export.py ==========

# yolov5/export.pyç¬¬160è¡Œ
# output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output0']
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#         elif isinstance(model, DetectionModel):
#             dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(model, SegmentationModel) else ['output']            
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1,25200,85)
```

3. å¯¼å‡º onnx æ¨¡å‹

```shell
cd yolov5
python export.py --weights=yolov5s.pt --dynamic --simplify --include=onnx --opset=11
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp yolov5/yolov5s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8

# ä¿®æ”¹ä»£ç åœ¨ src/application/app_yolo.cpp: app_yolo å‡½æ•°ä¸­, ä½¿ç”¨ V5 çš„æ–¹å¼å³å¯è¿è¡Œ
# test(Yolo::Type::V5, TRT::Mode::FP32, "yolov5s");

make yolo -j64
```

</details>

<details>
<summary>YOLOv6æ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv6

```shell
git clone https://github.com/meituan/YOLOv6.git
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batchï¼Œå¹¶å»é™¤ anchor ç»´åº¦


```python
# ========== export_onnx.py ==========

# YOLOv6/deploy/ONNX/export_onnx.pyç¬¬84è¡Œ
# output_axes = {
#     'outputs': {0: 'batch'},
# }
# ä¿®æ”¹ä¸ºï¼š

output_axes = {
    'output': {0: 'batch'},
}

# YOLOv6/deploy/ONNX/export_onnx.pyç¬¬106è¡Œ
# torch.onnx.export(model, img, f, verbose=False, opset_version=13,
#                     training=torch.onnx.TrainingMode.EVAL,
#                     do_constant_folding=True,
#                     input_names=['images'],
#                     output_names=['num_dets', 'det_boxes', 'det_scores', 'det_classes']
#                     if args.end2end else ['outputs'],
#                     dynamic_axes=dynamic_axes)
# ä¿®æ”¹ä¸ºï¼š

torch.onnx.export(model, img, f, verbose=False, opset_version=13,
                    training=torch.onnx.TrainingMode.EVAL,
                    do_constant_folding=True,
                    input_names=['images'],
                    output_names=['num_dets', 'det_boxes', 'det_scores', 'det_classes']
                    if args.end2end else ['output'],
                    dynamic_axes=dynamic_axes)

# æ ¹æ®ä¸åŒçš„ head å»é™¤ anchor ç»´åº¦
# ========== effidehead_distill_ns.py ==========
# YOLOv6/yolov6/models/heads/effidehead_distill_ns.pyç¬¬141è¡Œ
# return torch.cat(
#     [
#         pred_bboxes,
#         torch.ones((b, pred_bboxes.shape[1], 1), device=pred_bboxes.device, dtype=pred_bboxes.dtype),
#         cls_score_list
#     ],
#     axis=-1)
# ä¿®æ”¹ä¸ºï¼š
return torch.cat(
    [
        pred_bboxes,
        cls_score_list
    ],
    axis=-1)

# ========== effidehead_fuseab.py ==========
# YOLOv6/yolov6/models/heads/effidehead_fuseab.pyç¬¬191è¡Œ
# return torch.cat(
#     [
#         pred_bboxes,
#         torch.ones((b, pred_bboxes.shape[1], 1), device=pred_bboxes.device, dtype=pred_bboxes.dtype),
#         cls_score_list
#     ],
#     axis=-1)
# ä¿®æ”¹ä¸ºï¼š
return torch.cat(
    [
        pred_bboxes,
        cls_score_list
    ],
    axis=-1)

# ========== effidehead_lite.py ==========
# YOLOv6/yolov6/models/heads/effidehead_lite.pyç¬¬123è¡Œ
# return torch.cat(
#     [
#         pred_bboxes,
#         torch.ones((b, pred_bboxes.shape[1], 1), device=pred_bboxes.device, dtype=pred_bboxes.dtype),
#         cls_score_list
#     ],
#     axis=-1)
# ä¿®æ”¹ä¸ºï¼š
return torch.cat(
    [
        pred_bboxes,
        cls_score_list
    ],
    axis=-1)
```

3. å¯¼å‡º onnx æ¨¡å‹

```shell
cd YOLOv6
python deploy/ONNX/export_onnx.py --weights yolov6s.pt --img 640 --dynamic-batch --simplify
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp YOLOv6/yolov6s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8

# ä¿®æ”¹ä»£ç åœ¨ src/application/app_yolo.cpp: app_yolo å‡½æ•°ä¸­, ä½¿ç”¨ V6 çš„æ–¹å¼å³å¯è¿è¡Œ
# test(Yolo::Type::V6, TRT::Mode::FP32, "yolov6s");

make yolo -j64
```
</details>

<details>
<summary>YOLOv7æ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv7

```shell
git clone https://github.com/WongKinYiu/yolov7.git 
```

2. å¯¼å‡º onnx æ¨¡å‹


```shell
python export.py --dynamic-batch --grid --simplify --weights=yolov7.pt
```

3. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp yolov7/yolov7.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8

# ä¿®æ”¹ä»£ç åœ¨ src/application/app_yolo.cpp: app_yolo å‡½æ•°ä¸­, ä½¿ç”¨ V7 çš„æ–¹å¼å³å¯è¿è¡Œ
# test(Yolo::Type::V7, TRT::Mode::FP32, "yolov7");

make yolo -j64
```

</details>

<details>
<summary>YOLOv8æ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv8

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬72è¡Œï¼Œforwardå‡½æ•°
# return y if self.export else (y, x)
# ä¿®æ”¹ä¸ºï¼š

return y.permute(0, 2, 1) if self.export else (y, x)

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬323è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹, åœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
# ========== export.py ==========
from ultralytics import YOLO

model = YOLO("yolov8s.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolov8s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo -j64
```
</details>

<details>
<summary>YOLOv8-Clsæ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv8

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬323è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    dynamic['output'] = {0: 'batch'}
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹, åœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
# ========== export.py ==========
from ultralytics import YOLO

model = YOLO("yolov8s-cls.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolov8s-cls.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_cls -j64
```
</details>

<details>
<summary>YOLOv8-Segæ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv8

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬106è¡Œï¼Œforwardå‡½æ•°
# return (torch.cat([x, mc], 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))
# ä¿®æ”¹ä¸ºï¼š

return (torch.cat([x, mc], 1).permute(0, 2, 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬323è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹, åœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
# ========== export.py ==========
from ultralytics import YOLO

model = YOLO("yolov8s-seg.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolov8s-seg.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_seg -j64
```
</details>

<details>
<summary>YOLOv8-OBBæ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv8

```shell
glit clone https://github.com/ultralytics/ultralytics.git
cd ultralytics
git checkout tags/v8.1.0 -b v8.1.0
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬141è¡Œï¼Œforwardå‡½æ•°
# return torch.cat([x, angle], 1) if self.export else (torch.cat([x[0], angle], 1), (x[1], angle))
# ä¿®æ”¹ä¸ºï¼š

return torch.cat([x, angle], 1).permute(0, 2, 1) if self.export else (torch.cat([x[0], angle], 1), (x[1], angle))

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬353è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹, åœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
# ========== export.py ==========
from ultralytics import YOLO

model = YOLO("yolov8s-obb.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolov8s-obb.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_obb -j64
```

</details>

<details>
<summary>YOLOv8-Poseæ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv8

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬130è¡Œï¼Œforwardå‡½æ•°
# return torch.cat([x, pred_kpt], 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))
# ä¿®æ”¹ä¸ºï¼š

return torch.cat([x, pred_kpt], 1).permute(0, 2, 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬323è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    dynamic['output'] = {0: 'batch'}
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹, åœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
# ========== export.py ==========
from ultralytics import YOLO

model = YOLO("yolov8s-pose.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolov8s-pose.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_pose -j64
```
</details>

<details>
<summary>RT-DETRæ”¯æŒ</summary>

1. å‰ç½®æ¡ä»¶

- **tensorRT >= 8.6**

2. ä¸‹è½½ YOLOv8

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

3. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬323è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1, 84, 8400)
```

4. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼ˆå¯èƒ½ä¼šç”±äº torch ç‰ˆæœ¬é—®é¢˜å¯¼å‡ºå¤±è´¥, å…·ä½“å¯å‚è€ƒ [#6144](https://github.com/ultralytics/ultralytics/issues/6144)ï¼‰

```python
from ultralytics import RTDETR

model = RTDETR("rtdetr-l.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

5. engine ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šæ›¿æ¢ tensorRT_Pro-YOLOv8 ä¸­çš„ onnxparser è§£æå™¨ï¼Œå…·ä½“å¯å‚è€ƒæ–‡ç« ï¼š[RT-DETRæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134356250)
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engine

```shell
cp ultralytics/yolov8s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8/workspace
bash build.sh
```

6. æ‰§è¡Œ

```shell
make rtdetr -j64
```

</details>


<details>
<summary>ByteTrackæ”¯æŒ</summary>

1. è¯´æ˜

ä»£ç  copy è‡ªï¼š[https://github.com/CYYAI/AiInfer/tree/main/utils/tracker/ByteTracker](https://github.com/CYYAI/AiInfer/tree/main/utils/tracker/ByteTracker)

ä»¥ YOLOv8 ä½œä¸ºæ£€æµ‹å™¨å®ç°åŸºæœ¬è·Ÿè¸ªåŠŸèƒ½ï¼ˆå…¶å®ƒæ£€æµ‹å™¨ä¹Ÿè¡Œï¼‰

2. demo æ¼”ç¤º

```shell
cd tensorRT_Pro-YOLOv8
make bytetrack -j64
```

</details>

<details>
<summary>YOLOv9æ”¯æŒ</summary>

1. è¯´æ˜
   
æœ¬é¡¹ç›®çš„ YOLOv9 éƒ¨ç½²å®ç°å¹¶ä¸æ˜¯å®˜æ–¹åŸç‰ˆï¼Œè€Œæ˜¯é‡‡ç”¨çš„é›†æˆåˆ° ultralytics çš„ YOLOv9

2. ä¸‹è½½ YOLOv8

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

3. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬75è¡Œï¼Œforwardå‡½æ•°
# return y if self.export else (y, x)
# ä¿®æ”¹ä¸ºï¼š

return y.permute(0, 2, 1) if self.export else (y, x)

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬365è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1, 84, 8400)
```

4. å¯¼å‡º onnx æ¨¡å‹, åœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
# ========== export.py ==========
from ultralytics import YOLO

model = YOLO("yolov9c.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

5. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolov9c.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo -j64
```
</details>

<details>

<summary>YOLOv10æ”¯æŒ</summary>

1. å‰ç½®æ¡ä»¶

- **tensorRT >= 8.5**

2. ä¸‹è½½ YOLOv10

```shell
git clone https://github.com/THU-MIG/yolov10
```

3. ä¿®æ”¹ä»£ç , ä¿è¯åŠ¨æ€ batch

```python
# ========== exporter.py ==========

# yolov10-main/ultralytics/engine/exporter.pyç¬¬323è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1, 84, 8400)
```

4. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ yolov10-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹

```python
from ultralytics import YOLO

model = YOLO("yolov10s.pt")

success = model.export(format="onnx", dynamic=True, simplify=True, opset=13)
```

```shell
cd yolov10-main
python export.py
```

5. engine ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šæ›¿æ¢ tensorRT_Pro-YOLOv8 ä¸­çš„ onnxparser è§£æå™¨ï¼Œå…·ä½“å¯å‚è€ƒæ–‡ç« ï¼š[RT-DETRæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134356250)
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engine

```shell
cp yolov10-main/yolov10s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8/workspace
# å–æ¶ˆ build.sh ä¸­ yolov10 engine ç”Ÿæˆçš„æ³¨é‡Š
bash build.sh
```

6. æ‰§è¡Œ

```shell
make yolo -j64
```

</details>

<details>

<summary>RTMOæ”¯æŒ</summary>

1. å‰ç½®æ¡ä»¶

- **tensorRT >= 8.6**

2. RTMO å¯¼å‡ºç¯å¢ƒæ­å»º

```shell
conda create -n mmpose python=3.9
conda activate mmpose
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.0rc2"
mim install "mmpose>=1.1.0"
pip install mmdeploy==1.3.1
pip install mmdeploy-runtime==1.3.1
```

3. é¡¹ç›®å…‹éš†

```shell
git clone https://github.com/open-mmlab/mmpose.git
```   

4. é¢„è®­ç»ƒæƒé‡ä¸‹è½½

- å‚è€ƒï¼š[https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo-model-zoo](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo#%EF%B8%8F-model-zoo)

5. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ mmpose-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
import torch
from mmpose.apis import init_model
from mmpose.structures.bbox import bbox_xyxy2cs

class MyModel(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.model = init_model(config_file, checkpoint_file, device=device)
        test_cfg = {'input_size': (640, 640)}
        self.model.neck.switch_to_deploy(test_cfg)
        self.model.head.switch_to_deploy(test_cfg)
        self.model.head.dcc.switch_to_deploy(test_cfg)

    def forward(self, x):
        x = self.model.backbone(x)
        x = self.model.neck(x)
        cls_scores, bbox_preds, _, kpt_vis, pose_vecs = self.model.head(x)[:5]
        scores = self.model.head._flatten_predictions(cls_scores).sigmoid()
        flatten_bbox_preds = self.model.head._flatten_predictions(bbox_preds)
        flatten_pose_vecs  = self.model.head._flatten_predictions(pose_vecs)
        flatten_kpt_vis    = self.model.head._flatten_predictions(kpt_vis).sigmoid()
        bboxes = self.model.head.decode_bbox(flatten_bbox_preds, self.model.head.flatten_priors,
                                             self.model.head.flatten_stride)
        dets      = torch.cat([bboxes, scores], dim=2)
        grids     = self.model.head.flatten_priors
        bbox_cs   = torch.cat(bbox_xyxy2cs(dets[..., :4], self.model.head.bbox_padding), dim=-1)
        keypoints = self.model.head.dcc.forward_test(flatten_pose_vecs, bbox_cs, grids)
        pred_kpts = torch.cat([keypoints, flatten_kpt_vis.unsqueeze(-1)], dim=-1)
        bs, bboxes, ny, nx = map(int, pred_kpts.shape)
        bs = -1
        pred_kpts = pred_kpts.view(bs, bboxes, ny*nx)
        return torch.cat([dets, pred_kpts], dim=2)

if __name__ == "__main__":

    device = "cpu"
    config_file     = "configs/body_2d_keypoint/rtmo/body7/rtmo-s_8xb32-600e_body7-640x640.py"
    checkpoint_file = "rtmo-s_8xb32-600e_body7-640x640-dac2bf74_20231211.pth"

    model = MyModel()
    model.eval()

    x = torch.zeros(1, 3, 640, 640, device=device)
    dynamic_batch = {'images': {0: 'batch'}, 'output': {0: 'batch'}}
    torch.onnx.export(
        model,
        (x,),
        "rtmo-s_8xb32-600e_body7-640x640.onnx",
        input_names=["images"],
        output_names=["output"],
        opset_version=17,
        dynamic_axes=dynamic_batch
    )

    # Checks
    import onnx
    model_onnx = onnx.load("rtmo-s_8xb32-600e_body7-640x640.onnx")
    # onnx.checker.check_model(model_onnx)    # check onnx model

    # Simplify
    try:
        import onnxsim

        print(f"simplifying with onnxsim {onnxsim.__version__}...")
        model_onnx, check = onnxsim.simplify(model_onnx)
        assert check, "Simplified ONNX model could not be validated"
    except Exception as e:
        print(f"simplifier failure: {e}")

    onnx.save(model_onnx, "rtmo-s_8xb32-600e_body7-640x640.onnx")
    print(f"simplify done.")
```

```shell
cd mmpose-main
conda activate mmpose
python export.py
```

6. engien ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šæ›¿æ¢ tensorRT_Pro-YOLOv8 ä¸­çš„ onnxparser è§£æå™¨ï¼Œå…·ä½“å¯å‚è€ƒæ–‡ç« ï¼š[RT-DETRæ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°](https://blog.csdn.net/qq_40672115/article/details/134356250)
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engine

```shell
cp mmpose/rtmo-s_8xb32-600e_body7-640x640.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8/workspace
# å–æ¶ˆ build.sh ä¸­ rtmo engine ç”Ÿæˆçš„æ³¨é‡Š
bash build.sh
```

7. æ‰§è¡Œ

```shell
make rtmo -j64
```

</details>

<details>

<summary>LayerNorm Pluginæ”¯æŒ</summary>

1. è¯´æ˜

* å½“éœ€è¦åœ¨ä½ç‰ˆæœ¬çš„ tensorRT ä¸­è§£æ LayerNorm ç®—å­æ—¶å¯ä»¥é€šè¿‡è¯¥æ’ä»¶æ”¯æŒ
* LayerNorm æ’ä»¶å®ç°ä»£ç  copy è‡ª [CUDA-BEVFusion/src/plugins/custom_layernorm.cu](https://github.com/NVIDIA-AI-IOT/Lidar_AI_Solution/blob/master/CUDA-BEVFusion/src/plugins/custom_layernorm.cu)ï¼Œä»£ç è¿›è¡Œäº†ç•¥å¾®ä¿®æ”¹
* LayerNorm æ’ä»¶çš„å°è£…åœ¨æ¨ç†æ—¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå› æ­¤å¹¶æœªä½¿ç”¨

2. libcustom_layernorm.so ç”Ÿæˆ

```shell
cd tensorRT_Pro-YOLOv8
mkdir build && cd build
cmake .. && make -j64
cp libcustom_layernorm.so ../workspace
```

3. ONNX æ¨¡å‹ä¿®æ”¹ï¼ˆRTMO ä¸ºä¾‹è¯´æ˜ï¼Œå…¶å®ƒæ¨¡å‹ç±»ä¼¼ï¼‰

åˆ©ç”¨ onnx_graphsurgeon ä¿®æ”¹åŸå§‹ LayerNorm çš„ op_typeï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
import onnx
import onnx_graphsurgeon as gs

# åŠ è½½ ONNX æ¨¡å‹
input_model_path = "rtmo-s_8xb32-600e_body7-640x640.onnx"
output_model_path = "rtmo-s_8xb32-600e_body7-640x640.plugin.onnx"
graph = gs.import_onnx(onnx.load(input_model_path))

# éå†å›¾ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
for node in graph.nodes:
    if node.op == "LayerNormalization":
        node.op = "CustomLayerNormalization"
        # æ·»åŠ è‡ªå®šä¹‰å±æ€§
        node.attrs["name"] = "LayerNormPlugin"
        node.attrs["info"] = "This is custom LayerNormalization node"

# åˆ é™¤æ— ç”¨çš„èŠ‚ç‚¹å’Œå¼ é‡
graph.cleanup()

# å¯¼å‡ºä¿®æ”¹åçš„æ¨¡å‹
onnx.save(gs.export_onnx(graph), output_model_path)
```

4. engine ç”Ÿæˆ

åˆ©ç”¨ **trtexec** å·¥å…·åŠ è½½æ’ä»¶è§£æ ONNXï¼Œæ–°å»º build.sh è„šæœ¬æ–‡ä»¶å¹¶æ‰§è¡Œï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```shell
#! /usr/bin/bash

TRTEXEC=/home/jarvis/lean/TensorRT-8.5.1.7/bin/trtexec

# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/jarvis/lean/TensorRT-8.5.1.7/lib

${TRTEXEC} \
  --onnx=rtmo-s_8xb32-600e_body7-640x640.plugin.onnx \
  --plugins=libcustom_layernorm.so \
  --minShapes=images:1x3x640x640 \
  --optShapes=images:1x3x640x640 \
  --maxShapes=images:4x3x640x640 \
  --memPoolSize=workspace:2048 \
  --saveEngine=rtmo-s_8xb32-600e_body7-640x640.plugin.FP32.trtmodel \
  > trtexec_output.log 2>&1
```

</details>

<details>
<summary>PP-OCRv4æ”¯æŒ</summary>

1. å¯¼å‡ºç¯å¢ƒæ­å»º

```shell
conda create --name paddleocr python=3.9
conda activate paddleocr
pip install shapely scikit-image imgaug pyclipper lmdb tqdm numpy==1.26.4 rapidfuzz onnxruntime
pip install "opencv-python<=4.6.0.66" "opencv-contrib-python<=4.6.0.66" cython "Pillow>=10.0.0" pyyaml requests
pip install paddlepaddle paddleocr paddle2onnx
```

2. é¡¹ç›®å…‹éš†

```shell
git clone https://github.com/PaddlePaddle/PaddleOCR.git
```

3. é¢„è®­ç»ƒæƒé‡ä¸‹è½½

- å‚è€ƒï¼š[ğŸ› ï¸ PP-OCR ç³»åˆ—æ¨¡å‹åˆ—è¡¨ï¼ˆæ›´æ–°ä¸­ï¼‰](https://github.com/PaddlePaddle/PaddleOCR?tab=readme-ov-file#%EF%B8%8F-pp-ocr-%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E5%88%97%E8%A1%A8%E6%9B%B4%E6%96%B0%E4%B8%AD)

4. å¯¼å‡º onnx æ¨¡å‹ï¼Œå…·ä½“æµç¨‹è¯·å‚è€ƒï¼š[PaddleOCR-PP-OCRv4æ¨ç†è¯¦è§£åŠéƒ¨ç½²å®ç°ï¼ˆä¸Šï¼‰](https://blog.csdn.net/qq_40672115/article/details/140571346)

5. engine ç”Ÿæˆ
   
- **æ–¹æ¡ˆä¸€**ï¼šåˆ©ç”¨ **TRT::compile** æ¥å£ï¼ŒHardSwish ç®—å­è§£æé—®é¢˜å¯ä»¥é€šè¿‡æ’ä»¶æˆ–è€…æ›¿æ¢ onnxparser è§£æå™¨è§£å†³
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engine (**recommend**)

```shell
cd tensorRT_Pro-YOLOv8/workspace
bash ocr_build.sh
```

6. æ‰§è¡Œ

```shell
make ppocr -j64
```

</details>

<details>
<summary>LaneATTæ”¯æŒ</summary>

1. å¯¼å‡ºç¯å¢ƒæ­å»º

```shell
conda create -n laneatt python=3.10
conda activate laneatt
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2
pip install pyyaml opencv-python scipy imgaug numpy==1.26.4 tqdm p_tqdm ujson scikit-learn tensorboard
pip install onnx onnxruntime onnx-simplifier
```

2. é¡¹ç›®å…‹éš†

```shell
git clone https://github.com/lucastabelini/LaneATT.git
```

3. é¢„è®­ç»ƒæƒé‡ä¸‹è½½

```shell
gdown "https://drive.google.com/uc?id=1R638ou1AMncTCRvrkQY6I-11CPwZy23T" # main experiments on TuSimple, CULane and LLAMAS (1.3 GB)
unzip laneatt_experiments.zip
```

4. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ laneatt-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
import torch
from lib.models.laneatt import LaneATT

class LaneATTONNX(torch.nn.Module):
    def __init__(self, model):
        super(LaneATTONNX, self).__init__()
        # Params
        self.fmap_h = model.fmap_h  # 11
        self.fmap_w = model.fmap_w  # 20
        self.anchor_feat_channels = model.anchor_feat_channels  # 64
        self.anchors = model.anchors
        self.cut_xs = model.cut_xs
        self.cut_ys = model.cut_ys
        self.cut_zs = model.cut_zs
        self.invalid_mask = model.invalid_mask
        # Layers
        self.feature_extractor = model.feature_extractor
        self.conv1 = model.conv1
        self.cls_layer = model.cls_layer
        self.reg_layer = model.reg_layer
        self.attention_layer = model.attention_layer

        # Exporting the operator eye to ONNX opset version 11 is not supported
        attention_matrix = torch.eye(1000)
        self.non_diag_inds = torch.nonzero(attention_matrix == 0., as_tuple=False)
        self.non_diag_inds = self.non_diag_inds[:, 1] + 1000 * self.non_diag_inds[:, 0]  # 999000

        self.anchor_parts_1 = self.anchors[:, 2:4]
        self.anchor_parts_2 = self.anchors[:, 4:]

    def forward(self, x):
        batch_features = self.feature_extractor(x)
        batch_features = self.conv1(batch_features)
        # batch_anchor_features = self.cut_anchor_features(batch_features)
        # batchx15360
        batch_anchor_features = batch_features.reshape(-1, int(batch_features.numel()))
        # h, w = batch_features.shape[2:4]  # 12, 20
        indices = self.cut_xs + 20 * self.cut_ys + 12 * 20 * self.cut_zs        
        batch_anchor_features = batch_anchor_features[:, indices].\
            view(-1, 1000, self.anchor_feat_channels, self.fmap_h, 1)        
        # batch_anchor_features[self.invalid_mask] = 0
        batch_anchor_features = batch_anchor_features * torch.logical_not(self.invalid_mask)

        # Join proposals from all images into a single proposals features batch
        # batchx1000x704
        batch_anchor_features = batch_anchor_features.view(-1, 1000, self.anchor_feat_channels * self.fmap_h)

        # Add attention features
        softmax = torch.nn.Softmax(dim=2)
        # batchx1000x999
        scores = self.attention_layer(batch_anchor_features)
        attention = softmax(scores)
        # bs, _, _ = scores.shape
        bs, _, _ =scores.shape
        attention_matrix = torch.zeros(bs, 1000 * 1000, device=x.device)
        attention_matrix[:, self.non_diag_inds] = attention.reshape(-1, int(attention.numel()))
        attention_matrix = attention_matrix.view(-1, 1000, 1000)
        attention_features = torch.matmul(torch.transpose(batch_anchor_features, 1, 2),
                                          torch.transpose(attention_matrix, 1, 2)).transpose(1, 2)
        batch_anchor_features = torch.cat((attention_features, batch_anchor_features), dim=2)

        # Predict
        cls_logits = self.cls_layer(batch_anchor_features)
        reg = self.reg_layer(batch_anchor_features)

        anchor_expanded_1 = self.anchor_parts_1.repeat(reg.shape[0], 1, 1)
        anchor_expanded_2 = self.anchor_parts_2.repeat(reg.shape[0], 1, 1)  

        # Add offsets to anchors (1000, 2+2+73)
        reg_proposals = torch.cat([softmax(cls_logits), anchor_expanded_1, anchor_expanded_2 + reg], dim=2)

        return reg_proposals

def export_onnx(onnx_file_path):
    # e.g. laneatt_r18_culane
    backbone_name = 'resnet18'
    checkpoint_file_path = 'experiments/laneatt_r18_culane/models/model_0015.pt'
    anchors_freq_path = 'data/culane_anchors_freq.pt'

    # Load specified checkpoint
    model = LaneATT(backbone=backbone_name, anchors_freq_path=anchors_freq_path, topk_anchors=1000)
    checkpoint = torch.load(checkpoint_file_path)
    model.load_state_dict(checkpoint['model'])
    model.eval()

    # Export to ONNX
    onnx_model = LaneATTONNX(model)
    
    dummy_input = torch.randn(1, 3, 360, 640)
    dynamic_batch = {'images': {0: 'batch'}, 'output': {0: 'batch'}}
    torch.onnx.export(
        onnx_model, 
        dummy_input, 
        onnx_file_path, 
        input_names=["images"], 
        output_names=["output"],
        dynamic_axes=dynamic_batch
    )

    import onnx
    model_onnx = onnx.load(onnx_file_path)

    # Simplify
    try:
        import onnxsim

        print(f"simplifying with onnxsim {onnxsim.__version__}...")
        model_onnx, check = onnxsim.simplify(model_onnx)
        assert check, "Simplified ONNX model could not be validated"
    except Exception as e:
        print(f"simplifier failure: {e}")

    onnx.save(model_onnx, "laneatt.sim.onnx")
    print(f"simplify done. onnx model save in laneatt.sim.onnx")   

if __name__ == '__main__':
    export_onnx('./laneatt.onnx')
```

```shell
cd laneatt-main
conda activate laneatt
python export.py
```

5. engine ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šåˆ©ç”¨ **TRT::compile** æ¥å£ï¼ŒScatterND ç®—å­è§£æé—®é¢˜å¯ä»¥é€šè¿‡æ’ä»¶æˆ–è€…æ›¿æ¢ onnxparser è§£æå™¨è§£å†³
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engineï¼ˆ**recommend**ï¼‰

```shell
cd tensorRT_Pro-YOLOv8/workspace
bash lane_build.sh
```

</details>

<details>
<summary>CLRNetæ”¯æŒ</summary>

**1.** å‰ç½®æ¡ä»¶

- **tensorRT >= 8.6**

**2.** å¯¼å‡ºç¯å¢ƒæ­å»º

```shell
conda create -n clrnet python=3.9
conda activate clrnet
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2
pip install pandas addict scikit-learn opencv-python pytorch_warmup scikit-image tqdm p_tqdm
pip install imgaug yapf timm pathspec pthflops
pip install numpy==1.26.4 mmcv==1.2.5 albumentations==0.4.6 ujson==1.35 Shapely==2.0.5
pip install onnx onnx-simplifier onnxruntime
```

**3.** é¡¹ç›®å…‹éš†

```shell
git clone https://github.com/Turoad/CLRNet.git
```

**4.** é¢„è®­ç»ƒæƒé‡ä¸‹è½½

- ä¸‹è½½é“¾æ¥ï¼ˆ[Baidu Drive](https://pan.baidu.com/s/1rqXG6VXvzNeI-4Jl_vwKJQ?pwd=lane)ï¼‰

**5.** å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ clrnet-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
import math
import torch
import torch.nn.functional as F
from clrnet.utils.config import Config
from mmcv.parallel import MMDataParallel
from clrnet.models.registry import build_net

class CLRNetONNX(torch.nn.Module):
    def __init__(self, model):
        super(CLRNetONNX, self).__init__()
        self.backbone = model.backbone
        self.neck     = model.neck
        self.head     = model.heads

    def forward(self, x):
        x = self.backbone(x)
        x = self.neck(x)
        batch_features = list(x[len(x) - self.head.refine_layers:])
        # 1x64x10x25+1x64x20x50+1x64x40x100
        batch_features.reverse()
        batch_size = batch_features[-1].shape[0]

        # 1x192x78
        priors = self.head.priors.repeat(batch_size, 1, 1)
        # 1x192x36
        priors_on_featmap = self.head.priors_on_featmap.repeat(batch_size, 1, 1)
        
        prediction_lists = []
        prior_features_stages = []
        for stage in range(self.head.refine_layers):
            # 1. anchor ROI pooling
            num_priors = int(priors_on_featmap.shape[1])
            prior_xs = torch.flip(priors_on_featmap, dims=[2])
            batch_prior_features = self.head.pool_prior_features(
                batch_features[stage], num_priors, prior_xs)
            prior_features_stages.append(batch_prior_features)

            # 2. ROI gather
            fc_features = self.head.roi_gather(prior_features_stages, 
                                               batch_features[stage], stage)
            
            # 3. cls and reg head           
            # fc_features = fc_features.view(num_priors, batch_size, -1).reshape(batch_size * num_priors, self.head.fc_hidden_dim)
            fc_features = fc_features.view(num_priors, -1, 64).reshape(-1, self.head.fc_hidden_dim)
            
            cls_features = fc_features.clone()
            reg_features = fc_features.clone()
            for cls_layer in self.head.cls_modules:
                cls_features = cls_layer(cls_features)
            for reg_layer in self.head.reg_modules:
                reg_features = reg_layer(reg_features)
            
            cls_logits = self.head.cls_layers(cls_features)
            reg = self.head.reg_layers(reg_features)

            # cls_logits = cls_logits.reshape(batch_size, -1, cls_logits.shape[1]) # (B, num_priors, 2)
            cls_logits = cls_logits.reshape(-1, 192, 2) # (B, num_priors, 2)
            # add softmax
            softmax = torch.nn.Softmax(dim=2)
            cls_logits = softmax(cls_logits)
            # reg = reg.reshape(batch_size, -1, reg.shape[1])
            reg = reg.reshape(-1, 192, 76)
            
            predictions = priors.clone()
            predictions[:, :, :2] = cls_logits
            predictions[:, :, 2:5] += reg[:, :, :3]
            # add n_strips * length
            # predictions[:, :, 5] = reg[:, :, 3] # length
            predictions[:, :, 5] = reg[:, :, 3] * self.head.n_strips # length
            
            def tran_tensor(t):
                return t.unsqueeze(2).clone().repeat(1, 1, self.head.n_offsets)
            
            batch_size = reg.shape[0]
            predictions[..., 6:] = (
                tran_tensor(predictions[..., 3]) * (self.head.img_w - 1) +
                ((1 - self.head.prior_ys.repeat(batch_size, num_priors, 1) -
                  tran_tensor(predictions[..., 2])) * self.head.img_h /
                 torch.tan(tran_tensor(predictions[..., 4]) * math.pi + 1e-5))) / (self.head.img_w - 1)

            prediction_lines = predictions.clone()
            predictions[..., 6:] += reg[..., 4:]

            prediction_lists.append(predictions)

            if stage != self.head.refine_layers - 1:
                priors = prediction_lines.detach().clone()
                priors_on_featmap = priors[..., 6 + self.head.sample_x_indexs]

        return prediction_lists[-1]            
    
def export_onnx(onnx_file_path):
    # e.g. clrnet_culane_r18
    cfg = Config.fromfile("configs/clrnet/clr_resnet18_culane.py")
    checkpoint_file_path = "culane_r18.pth"
    # load checkpoint
    net = build_net(cfg)
    net = MMDataParallel(net, device_ids=range(1)).cuda()
    pretrained_model = torch.load(checkpoint_file_path)
    net.load_state_dict(pretrained_model['net'], strict=False)
    net.eval()
    model = net.to("cpu")

    onnx_model = CLRNetONNX(model.module)
    # Export to ONNX
    dummy_input = torch.randn(1, 3 ,320, 800)
    dynamic_batch = {'images': {0: 'batch'}, 'output': {0: 'batch'}}
    torch.onnx.export(
        onnx_model,
        dummy_input,
        onnx_file_path,
        input_names=["images"],
        output_names=["output"],
        opset_version=17,
        dynamic_axes=dynamic_batch
    )
    print(f"finished export onnx model")

    import onnx
    model_onnx = onnx.load(onnx_file_path)
    onnx.checker.check_model(model_onnx)    # check onnx model

    # Simplify
    try:
        import onnxsim

        print(f"simplifying with onnxsim {onnxsim.__version__}...")
        model_onnx, check = onnxsim.simplify(model_onnx)
        assert check, "Simplified ONNX model could not be validated"
    except Exception as e:
        print(f"simplifier failure: {e}")

    onnx.save(model_onnx, "clrnet.sim.onnx")
    print(f"simplify done. onnx model save in clrnet.sim.onnx")
    
if __name__ == "__main__":
    export_onnx("./clrnet.onnx")
```

```shell
cd clrnet-main
conda activate clrnet
python export.py
```

**5.** engine ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šåˆ©ç”¨ **TRT::compile** æ¥å£ï¼ŒGridSample å’Œ LayerNormalization ç®—å­è§£æé—®é¢˜å¯ä»¥é€šè¿‡æ’ä»¶æˆ–è€…æ›¿æ¢ onnxparser è§£æå™¨è§£å†³
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engineï¼ˆ**recommend**ï¼‰

```shell
cd tensorRT_Pro-YOLOv8/workspace
bash lane_build.sh
```

</details>

<details>
<summary>CLRerNetæ”¯æŒ</summary>

**1.** å‰ç½®æ¡ä»¶

- **tensorRT >= 8.6**

**2.** å¯¼å‡ºç¯å¢ƒæ­å»º

```shell
conda create -n clrernet python=3.8
conda activate clrernet
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2
pip install -U openmim==0.3.3
mim install mmcv-full==1.7.0
pip install albumentations==0.4.6 p_tqdm==1.3.3 yapf==0.40.1 mmdet==2.28.0
pip install pytest pytest-cov tensorboard
pip install onnx onnx-simplifier onnxruntime
```

**3.** é¡¹ç›®å…‹éš†

```shell
git clone https://github.com/hirotomusiker/CLRerNet.git
```

**4.** é¢„è®­ç»ƒæƒé‡ä¸‹è½½

- ä¸‹è½½é“¾æ¥ï¼ˆ[Baidu Drive](https://pan.baidu.com/s/1_rszDtajwTpvH1O_OPFR9A?pwd=lane)ï¼‰

**5.** å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ clrernet-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
import torch
from mmcv import Config
from mmdet.models import build_detector
from mmcv.runner import load_checkpoint

class CLRerNetONNX(torch.nn.Module):
    def __init__(self, model):
        super(CLRerNetONNX, self).__init__()
        self.model = model
        self.bakcbone = model.backbone
        self.neck     = model.neck
        self.head     = model.bbox_head

    def forward(self, x):
        x = self.bakcbone(x)
        x = self.neck(x)
        
        batch = x[0].shape[0]
        feature_pyramid = list(x[len(x) - self.head.refine_layers:])
        # 1x64x10x25+1x64x20x50+1x64x40x100
        feature_pyramid.reverse()
        
        _, sampled_xs = self.head.anchor_generator.generate_anchors(
            self.head.anchor_generator.prior_embeddings.weight,
            self.head.prior_ys,
            self.head.sample_x_indices,
            self.head.img_w,
            self.head.img_h
        )

        anchor_params = self.head.anchor_generator.prior_embeddings.weight.clone().repeat(batch, 1, 1)
        priors_on_featmap = sampled_xs.repeat(batch, 1, 1)

        predictions_list = []
        pooled_features_stages = []
        for stage in range(self.head.refine_layers):
            # 1. anchor ROI pooling
            prior_xs = priors_on_featmap
            pooled_features = self.head.pool_prior_features(feature_pyramid[stage], prior_xs)
            pooled_features_stages.append(pooled_features)

            # 2. ROI gather
            fc_features = self.head.attention(pooled_features_stages, feature_pyramid, stage)
            # fc_features = fc_features.view(self.head.num_priors, batch, -1).reshape(batch * self.head.num_priors, self.head.fc_hidden_dim)
            fc_features = fc_features.view(self.head.num_priors, -1, 64).reshape(-1, self.head.fc_hidden_dim)

            # 3. cls and reg head
            cls_features = fc_features.clone()
            reg_features = fc_features.clone()
            for cls_layer in self.head.cls_modules:
                cls_features = cls_layer(cls_features)
            for reg_layer in self.head.reg_modules:
                reg_features = reg_layer(reg_features)
            
            cls_logits = self.head.cls_layers(cls_features)
            # cls_logits = cls_logits.reshape(batch, -1, cls_logits.shape[1])
            cls_logits = cls_logits.reshape(-1, 192, 2)

            reg = self.head.reg_layers(reg_features)
            # reg = reg.reshape(batch, -1, reg.shape[1])
            reg = reg.reshape(-1, 192, 76)

            # 4. reg processing
            anchor_params += reg[:, :, :3]
            updated_anchor_xs, _ = self.head.anchor_generator.generate_anchors(
                anchor_params.view(-1, 3),
                self.head.prior_ys,
                self.head.sample_x_indices,
                self.head.img_w,
                self.head.img_h
            )
            # updated_anchor_xs = updated_anchor_xs.view(batch, self.head.num_priors, -1)
            updated_anchor_xs = updated_anchor_xs.view(-1, 192, 72)
            reg_xs = updated_anchor_xs + reg[..., 4:]

            # start_y, start_x, theta
            # some problem.
            # anchor_params[:, :, 0] = 1.0 - anchor_params[:, :, 0]
            # anchor_params_ = anchor_params.clone()
            # anchor_params_[:, :, 0] = 1.0 - anchor_params_[:, :, 0]
            # print(f"anchor_params.shape = {anchor_params_.shape}")

            softmax = torch.nn.Softmax(dim=2)
            cls_logits = softmax(cls_logits)
            reg[:, :, 3:4] = reg[:, :, 3:4] * self.head.n_strips
            predictions = torch.concat([cls_logits, anchor_params, reg[:, :, 3:4], reg_xs], dim=2)
            # predictions = torch.concat([cls_logits, anchor_params_, reg[:, :, 3:4], reg_xs], dim=2)

            predictions_list.append(predictions)

            if stage != self.head.refine_layers - 1:
                anchor_params = anchor_params.detach().clone()
                priors_on_featmap = updated_anchor_xs.detach().clone()[
                    ..., self.head.sample_x_indices
                ]
        
        return predictions_list[-1]

    
if __name__ == "__main__":

    cfg = Config.fromfile("configs/clrernet/culane/clrernet_culane_dla34.py")
    model = build_detector(cfg.model, test_cfg=cfg.get("test_cfg"))
    load_checkpoint(model, "clrernet_culane_dla34.pth", map_location="cpu")
        
    model.eval()
    model = model.to("cpu")
    
    # Export to ONNX
    onnx_model = CLRerNetONNX(model)

    dummy_input = torch.randn(1, 3, 320, 800)

    dynamic_batch = {'images': {0: 'batch'}, 'output': {0: 'batch'}}
    torch.onnx.export(
        onnx_model, 
        dummy_input,
        "model.onnx",
        input_names=["images"],
        output_names=["output"],
        opset_version=17,
        dynamic_axes=dynamic_batch
    )
    print(f"finished export onnx model")

    import onnx
    model_onnx = onnx.load("model.onnx")
    onnx.checker.check_model(model_onnx)    # check onnx model

    # Simplify
    try:
        import onnxsim

        print(f"simplifying with onnxsim {onnxsim.__version__}...")
        model_onnx, check = onnxsim.simplify(model_onnx)
        assert check, "Simplified ONNX model could not be validated"
    except Exception as e:
        print(f"simplifier failure: {e}")

    onnx.save(model_onnx, "clrernet.sim.onnx")
    print(f"simplify done. onnx model save in clrernet.sim.onnx")
```

```shell
cd clrernet-main
conda activate clrernet
python export.py
```

**5.** engine ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šåˆ©ç”¨ **TRT::compile** æ¥å£ï¼ŒGridSample å’Œ LayerNormalization ç®—å­è§£æé—®é¢˜å¯ä»¥é€šè¿‡æ’ä»¶æˆ–è€…æ›¿æ¢ onnxparser è§£æå™¨è§£å†³
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engineï¼ˆ**recommend**ï¼‰

```shell
cd tensorRT_Pro-YOLOv8/workspace
bash lane_build.sh
```

</details>

<details>
<summary>YOLO11æ”¯æŒ</summary>

1. ä¸‹è½½ YOLO11

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç ï¼Œä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬68è¡Œï¼Œforwardå‡½æ•°
# return y if self.export else (y, x)
# ä¿®æ”¹ä¸ºï¼š

return y.permute(0, 2, 1) if self.export else (y, x)

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬400è¡Œ
# output_names = ["output0", "output1"] if isinstance(self.model, SegmentationModel) else ["output0"]
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {"images": {0: "batch", 2: "height", 3: "width"}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 116, 8400)
#         dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ["output0", "output1"] if isinstance(self.model, SegmentationModel) else ["output"]
dynamic = self.args.dynamic
if dynamic:
    dynamic = {"images": {0: "batch"}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 116, 8400)
        dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic["output0"] = {0: "batch"}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
from ultralytics import YOLO

model = YOLO("yolo11s.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolo11s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo -j64
```

</details>

<details>
<summary>YOLO11-Clsæ”¯æŒ</summary>

1. ä¸‹è½½ YOLO11

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç ï¼Œä¿è¯åŠ¨æ€ batch

```python
# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬400è¡Œ
# output_names = ["output0", "output1"] if isinstance(self.model, SegmentationModel) else ["output0"]
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {"images": {0: "batch", 2: "height", 3: "width"}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 116, 8400)
#         dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ["output0", "output1"] if isinstance(self.model, SegmentationModel) else ["output"]
dynamic = self.args.dynamic
if dynamic:
    dynamic = {"images": {0: "batch"}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 116, 8400)
        dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic["output0"] = {0: "batch"}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
from ultralytics import YOLO

model = YOLO("yolo11s-cls.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolo11s-cls.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_cls -j64
```

</details>

<details>
<summary>YOLO11-Segæ”¯æŒ</summary>

1. ä¸‹è½½ YOLO11

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç ï¼Œä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬186è¡Œï¼Œforwardå‡½æ•°
# return (torch.cat([x, mc], 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))
# ä¿®æ”¹ä¸ºï¼š

return (torch.cat([x, mc], 1).permute(0, 2, 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬400è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
from ultralytics import YOLO

model = YOLO("yolo11s-seg.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolo11s-seg.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_seg -j64
```

</details>

<details>
<summary>YOLO11-OBBæ”¯æŒ</summary>

1. ä¸‹è½½ YOLO11

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç ï¼Œä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬212è¡Œï¼Œforwardå‡½æ•°
# return torch.cat([x, angle], 1) if self.export else (torch.cat([x[0], angle], 1), (x[1], angle))
# ä¿®æ”¹ä¸ºï¼š

return torch.cat([x, angle], 1).permute(0, 2, 1) if self.export else (torch.cat([x[0], angle], 1), (x[1], angle))

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬400è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output'] = {0: 'batch'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
from ultralytics import YOLO

model = YOLO("yolo11s-obb.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolo11s-obb.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_obb -j64
```

</details>

<details>
<summary>YOLO11-Poseæ”¯æŒ</summary>

1. ä¸‹è½½ YOLO11

```shell
git clone https://github.com/ultralytics/ultralytics.git
```

2. ä¿®æ”¹ä»£ç ï¼Œä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬239è¡Œï¼Œforwardå‡½æ•°
# return torch.cat([x, pred_kpt], 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))
# ä¿®æ”¹ä¸ºï¼š

return torch.cat([x, pred_kpt], 1).permute(0, 2, 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬400è¡Œ
# output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
#         dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output']
dynamic = self.args.dynamic
if dynamic:
    dynamic = {'images': {0: 'batch'}}  # shape(1,3,640,640)
    dynamic['output'] = {0: 'batch'}
    if isinstance(self.model, SegmentationModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 116, 8400)
        dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic['output0'] = {0: 'batch', 2: 'anchors'}  # shape(1, 84, 8400)
```

3. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ ultralytics-main æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
from ultralytics import YOLO

model = YOLO("yolo11s-pose.pt")

success = model.export(format="onnx", dynamic=True, simplify=True)
```

```shell
cd ultralytics-main
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp ultralytics/yolo11s-pose.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo_pose -j64
```

</details>

<details>

<summary>Depth-Anything-V1æ”¯æŒ</summary>

**1.** å‰ç½®æ¡ä»¶

- **tensorRT >= 8.6**

**2.** é¡¹ç›®å…‹éš†

```shell
git clone https://github.com/LiheYoung/Depth-Anything.git
```

**3.** é¢„è®­ç»ƒæƒé‡ä¸‹è½½

- ä¸‹è½½é“¾æ¥ï¼ˆ[Baidu Drive](https://pan.baidu.com/s/1VuNdh0N5afvpnaGqJ_Hnbw?pwd=1234)ï¼‰

**4.** ä¿®æ”¹ä»£ç ï¼Œä¿è¯æ­£ç¡®å¯¼å‡º

```python
# ========== dpt.py ==========

# depth_anything/dpt.pyç¬¬5è¡Œï¼Œæ³¨é‡Š
# from huggingface_hub import PyTorchModelHubMixin, hf_hub_download

# depth_anything/dpt.pyç¬¬166è¡Œï¼Œforwardå‡½æ•°
# return depth.squeeze(1)
# ä¿®æ”¹ä¸ºï¼š

return depth
```

**5.** å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ Depth-Anything é¡¹ç›®ä¸‹æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py`ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```python
import torch
import argparse
import torch.onnx
from depth_anything.dpt import DPT_DINOv2

def export_model(encoder: str, load_from: str, image_shape: tuple):

    # Initializing model
    assert encoder in ['vits', 'vitb', 'vitl']
    if encoder == 'vits':
        depth_anything = DPT_DINOv2(encoder='vits', features=64, out_channels=[48, 96, 192, 384], localhub='localhub')
    elif encoder == 'vitb':
        depth_anything = DPT_DINOv2(encoder='vitb', features=128, out_channels=[96, 192, 384, 768], localhub='localhub')
    else:
        depth_anything = DPT_DINOv2(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024], localhub='localhub')

    total_params = sum(param.numel() for param in depth_anything.parameters())
    print('Total parameters: {:.2f}M'.format(total_params / 1e6))

    # Loading model weight
    depth_anything.load_state_dict(torch.load(load_from, map_location='cpu'), strict=True)

    depth_anything.eval()

    # Define dummy input data
    dummy_input = torch.ones(image_shape).unsqueeze(0)

    onnx_path = load_from.split('/')[-1].split('.pth')[0] + '.onnx'

    dynamic_batch = {"images": {0: "batch"}, "output": {0: "batch"}}

    # Export the PyTorch model to ONNX format
    torch.onnx.export(
        depth_anything, 
        dummy_input, 
        onnx_path, 
        opset_version=17, 
        input_names=["images"], 
        output_names=["output"],
        dynamic_axes=None
    )

    import onnx
    model_onnx = onnx.load(onnx_path)

    # Simplify
    try:
        import onnxsim

        print(f"simplifying with onnxsim {onnxsim.__version__}...")
        model_onnx, check = onnxsim.simplify(model_onnx)
        assert check, "Simplified ONNX model could not be validated"
    except Exception as e:
        print(f"simplifier failure: {e}")

    onnx.save(model_onnx, f"depth_anything_{encoder}.sim.onnx")
    print(f"simplify done. onnx model save in depth_anything_{encoder}.sim.onnx")  

    print(f"Model exported to {onnx_path}")

def main():
    parser = argparse.ArgumentParser(description="Export Depth DPT model to ONNX format")
    parser.add_argument("--encoder", type=str, choices=['vits', 'vitb', 'vitl'], help="Type of encoder to use ('vits', 'vitb', 'vitl')")
    parser.add_argument("--load_from", type=str, help="Path to the pre-trained model checkpoint")
    parser.add_argument("--image_shape", type=int, nargs=3, metavar=("channels", "height", "width"), help="Shape of the input image")
    args = parser.parse_args()

    export_model(args.encoder, args.load_from, tuple(args.image_shape))

if __name__ == "__main__":
    main()
```

```shell
cd Depth-Anything
python export.py --encoder vits --load_from depth_anything_vits14.pth --image_shape 3 518 518
```

**6.** engine ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šåˆ©ç”¨ **TRT::compile** æ¥å£ï¼ŒLayerNormalization ç®—å­è§£æé—®é¢˜å¯ä»¥é€šè¿‡æ’ä»¶æˆ–è€…æ›¿æ¢ onnxparser è§£æå™¨è§£å†³
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engineï¼ˆ**recommend**ï¼‰

```shell
cd tensorRT_Pro-YOLOv8/workspace
bash depth_anything_build.sh
```

**7.** æ‰§è¡Œ

```shell
cd tensorRT_Pro-YOLOv8
make depth_anything -j64
```

</details>

<details>

<summary>Depth-Anything-V2æ”¯æŒ</summary>

**1.** å‰ç½®æ¡ä»¶

- **tensorRT >= 8.6**

**2.** é¡¹ç›®å…‹éš†

```shell
git clone https://github.com/DepthAnything/Depth-Anything-V2.git
```

**3.** é¢„è®­ç»ƒæƒé‡ä¸‹è½½

- ä¸‹è½½é“¾æ¥ï¼ˆ[Baidu Drive](https://pan.baidu.com/s/1VuNdh0N5afvpnaGqJ_Hnbw?pwd=1234)ï¼‰

**4.** ä¿®æ”¹ä»£ç ï¼Œä¿è¯æ­£ç¡®å¯¼å‡º

```python
# ========== dpt.py ==========

# depth_anything_v2/dpt.pyç¬¬184è¡Œï¼Œforwardå‡½æ•°
# return depth.squeeze(1)
# ä¿®æ”¹ä¸ºï¼š

return depth
```

**5.** å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ Depth-Anything-V2 é¡¹ç›®ä¸‹æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py`ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```python
import torch
import argparse
from depth_anything_v2.dpt import DepthAnythingV2

def main():
    parser = argparse.ArgumentParser(description='Depth Anything V2')
    
    parser.add_argument('--input-size', type=int, default=518)
    parser.add_argument('--encoder', type=str, default='vits', choices=['vits', 'vitb', 'vitl', 'vitg'])

    args = parser.parse_args()
    
    # we are undergoing company review procedures to release Depth-Anything-Giant checkpoint
    model_configs = {
        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
        'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
    }
    
    depth_anything = DepthAnythingV2(**model_configs[args.encoder])
    depth_anything.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_{args.encoder}.pth', map_location='cpu'))
    depth_anything = depth_anything.to('cpu').eval()

    # Define dummy input data
    dummy_input = torch.ones((3, args.input_size, args.input_size)).unsqueeze(0)

    onnx_path = f'depth_anything_v2_{args.encoder}.onnx'

    dynamic_batch = {"images": {0: "batch"}, "output": {0: "batch"}}
    
    # Export the PyTorch model to ONNX format
    torch.onnx.export(
        depth_anything, 
        dummy_input, 
        onnx_path, 
        opset_version=17, 
        input_names=["images"], 
        output_names=["output"],
        dynamic_axes=None
    )

    import onnx
    model_onnx = onnx.load(onnx_path)

    # Simplify
    try:
        import onnxsim

        print(f"simplifying with onnxsim {onnxsim.__version__}...")
        model_onnx, check = onnxsim.simplify(model_onnx)
        assert check, "Simplified ONNX model could not be validated"
    except Exception as e:
        print(f"simplifier failure: {e}")

    onnx.save(model_onnx, f"depth_anything_v2_{args.encoder}.sim.onnx")
    print(f"simplify done. onnx model save in depth_anything_v2_{args.encoder}.sim.onnx")  

if __name__ == "__main__":
    main()
```

```shell
cd Depth-Anything-V2
python export.py --encoder vits --input-size 518
```

**6.** engine ç”Ÿæˆ

- **æ–¹æ¡ˆä¸€**ï¼šåˆ©ç”¨ **TRT::compile** æ¥å£ï¼ŒLayerNormalization ç®—å­è§£æé—®é¢˜å¯ä»¥é€šè¿‡æ’ä»¶æˆ–è€…æ›¿æ¢ onnxparser è§£æå™¨è§£å†³
- **æ–¹æ¡ˆäºŒ**ï¼šåˆ©ç”¨ **trtexec** å·¥å…·ç”Ÿæˆ engineï¼ˆ**recommend**ï¼‰

```shell
cd tensorRT_Pro-YOLOv8/workspace
bash depth_anything_build.sh
```

**7.** æ‰§è¡Œ

```shell
cd tensorRT_Pro-YOLOv8
make depth_anything -j64
```

</details>

<details>
<summary>YOLOv12æ”¯æŒ</summary>

1. ä¸‹è½½ YOLOv12

```shell
git clone https://github.com/sunsmarterjie/yolov12
```

2. ä¿®æ”¹ä»£ç ï¼Œä¿è¯åŠ¨æ€ batch

```python
# ========== head.py ==========

# ultralytics/nn/modules/head.pyç¬¬74è¡Œï¼Œforwardå‡½æ•°
# return y if self.export else (y, x)
# ä¿®æ”¹ä¸ºï¼š

return y.permute(0, 2, 1) if self.export else (y, x)

# ========== exporter.py ==========

# ultralytics/engine/exporter.pyç¬¬499è¡Œ
# output_names = ["output0", "output1"] if isinstance(self.model, SegmentationModel) else ["output0"]
# dynamic = self.args.dynamic
# if dynamic:
#     dynamic = {"images": {0: "batch", 2: "height", 3: "width"}}  # shape(1,3,640,640)
#     if isinstance(self.model, SegmentationModel):
#         dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 116, 8400)
#         dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # shape(1,32,160,160)
#     elif isinstance(self.model, DetectionModel):
#         dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 84, 8400)
# ä¿®æ”¹ä¸ºï¼š

output_names = ["output0", "output1"] if isinstance(self.model, SegmentationModel) else ["output"]
dynamic = self.args.dynamic
if dynamic:
    dynamic = {"images": {0: "batch"}}  # shape(1,3,640,640)
    if isinstance(self.model, SegmentationModel):
        dynamic["output0"] = {0: "batch", 2: "anchors"}  # shape(1, 116, 8400)
        dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # shape(1,32,160,160)
    elif isinstance(self.model, DetectionModel):
        dynamic["output0"] = {0: "batch"}  # shape(1, 84, 8400)
```

1. å¯¼å‡º onnx æ¨¡å‹ï¼Œåœ¨ yolov12 æ–°å»ºå¯¼å‡ºæ–‡ä»¶ `export.py` å†…å®¹å¦‚ä¸‹ï¼š

```python
from ultralytics import YOLO

model = YOLO('yolov12s.pt')

model.export(format="onnx", dynamic=True)
```

```shell
cd yolov12
python export.py
```

4. å¤åˆ¶æ¨¡å‹å¹¶æ‰§è¡Œ

```shell
cp yolov12/yolov12s.onnx tensorRT_Pro-YOLOv8/workspace
cd tensorRT_Pro-YOLOv8
make yolo -j64
```

</details>

## æ¥å£ä»‹ç»

<details>
<summary>ç¼–è¯‘æ¥å£</summary>

```cpp
TRT::compile(
    mode,                       // FP32ã€FP16ã€INT8
    test_batch_size,            // max batch size
    onnx_file,                  // source 
    model_file,                 // save to
    {},                         // redefine the input shape
    int8process,                // the recall function for calibration
    "inference",                // the dir where the image data is used for calibration
    ""                          // the dir where the data generated from calibration is saved(a.k.a where to load the calibration data.)
);
```
* tensorRT_Pro åŸç¼–è¯‘æ¥å£, æ”¯æŒ FP32ã€FP16ã€INT8 ç¼–è¯‘
* æ¨¡å‹çš„ç¼–è¯‘å·¥ä½œä¹Ÿå¯ä»¥é€šè¿‡ `trtexec` å·¥å…·å®Œæˆ
</details>

<details>
<summary>æ¨ç†æ¥å£</summary>

```cpp
// åˆ›å»ºæ¨ç†å¼•æ“åœ¨ 0 å·æ˜¾å¡ä¸Š
auto engine = YoloPose::create_infer(
    engine_file,                    // engine file
    deviceid,                       // gpu id
    0.25f,                          // confidence threshold
    0.45f,                          // nms threshold
    YoloPose::NMSMethod::FastGPU,   // NMS method, fast GPU / CPU
    1024,                           // max objects
    false                           // preprocess use multi stream
);

// åŠ è½½å›¾åƒ
auto image = cv::imread("inference/car.jpg");

// æ¨ç†å¹¶è·å–ç»“æœ
auto boxes = engine->commit(image).get()  // å¾—åˆ°çš„æ˜¯ vector<Box>
```

</details>

## å‚è€ƒ
- [https://github.com/shouxieai/tensorRT_Pro](https://github.com/shouxieai/tensorRT_Pro)
- [https://github.com/shouxieai/infer](https://github.com/shouxieai/infer)
- [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)